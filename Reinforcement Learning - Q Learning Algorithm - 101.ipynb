{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Two most popular areas in Machine Learning are Supervised Learning & Unsupervised Learning. From solutions perspective the two areas are quite similar, as they basically give a single output given some data. But not all problems are so closely circuited and might be more complex. Lot of solutions are also deployed where complex problems are broken down to ML Pipelines, where different teams may be deployed to each component. The pipeline components are therefore generally independent of each other and talk to each other only as per solution defined APIs. Here we introduce Reinforcement Learning, which in a witty sense can be explained as Machine Learning Pipeline built using Machine Learning.   \n",
    "\n",
    "Reinforcement Learning is an emerging Machine Learning Area, where by we identify incremental \n",
    "decisions that may finally lead us to a success critria. This kind of application is most easily understood\n",
    "in the gaming arena. Some interesting games being studied in this area are:\n",
    "1. Go [3]\n",
    "2. Chess [4]\n",
    "3. Tic Tac Toe [2]\n",
    "4. Mario [5]\n",
    "and many more. \n",
    "\n",
    "The gaming industry is not the only one with reinforcement learning usecases, but all feilds with strategic intelligence requirements are expected to be greatly influenced by it. Be it Stock Trading, Autonomous Driving or maybe Robotic Surgeries (some day). If machines can become good at strategizing someday, it would really makes the jobs of many white collar executives obsolete.  \n",
    "\n",
    "Another way far-fetched yet interesting application of reinforcement learning can be to to create alter-images for people like one described at [6]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning Algorithm\n",
    "Now this is a pretty introductory session and my goal is to introduce a very simple and powerful reinforcement learning algorithm - Q Learning. To begin with all courtesies of this blog to Edureka for this excellent video on Youtube [1]. Let us try to learn a model that can come out of a simple maze. Here is the simple maze and a graph based representation of the same maze at https://github.com/HimanshuGautam17/Personal-Data-Repo/blob/master/Toy%20Maze.JPG (Not able to add a image in a Jupyter notebook hosted at github as of now ;)). \n",
    "\n",
    "This algorithm aims at coming out with best \"list of decisions\" to reach the end of maze given a random starting position in the graph. The algorithm depends on 2 matrices R & Q. \n",
    "R (Reward) matrix bascailly models the rules of the game. We create a square matrix of size equal to possible number of states and then define which states are favourable. So the states which directly lead to State \"5\" (out of maze) are given 100 points. All invalid state transitions are labelled -1 and other transitions are labelled as 0.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 6 dimensions for State 0 to 5\n",
    "R = np.matrix([[-1, -1, -1, -1, 0, -1],\n",
    "               [-1, -1, -1, 0, -1, 100],\n",
    "               [-1, -1, -1, 0, -1, -1],\n",
    "               [-1, 0, 0, -1, 0, -1],\n",
    "               [-1, 0, 0, -1, -1, 100],\n",
    "               [-1, 0, -1, -1, 0, 100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q matrix is a square matrix with dimension same as R. Here the incremental value of taking an action (i to j) is stored. This matrix is initially all zero and is filled based on the experience of agent by taking different paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q Matrix\n",
    "Q = np.zeros([6,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation vs Exploration (Hyper parameter)\n",
    "Gamma is a learning parameter that defines the explorative vs exploitative nature of the agent. A high gamma means agent gives more weightage to already learned Q matrix values (known paths / incremental reward), while low value means more weightage to exploring new paths. Here we set it as 0.8. The parameter does not seem to be much important for this toy example, but will be important for more complex problems. \n",
    "\n",
    "The method to update Q matrix is:<br><br>\n",
    "**Q(current_state, action) = R(current_state, action) + gamma * Max(Q(action, all valid next actions))**<br><br>\n",
    "The term R(current_state, action) represents explorative nature, where Reward is decided as per the rules of game. This value will be 0 if the action does not lead to state \"5\" and 100 otherwise. \n",
    "The Term Max(Q(action, all valid next actions)) represent the exploitative nature, where the reward for current action is determined by the max possible known reward from the state we would reach after taking the current action. So a path leading to known high rewards (based on experience) has a higher reward for itself.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gamma - Learning parameter. \n",
    "gamma = 0.8\n",
    "\n",
    "# Intial State - choosen randomly later on while training\n",
    "initial_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define some utility functions that returns list of valid actions from a initial_state. Choosing a random action from the valid actions and updating the Q matrix as per the gamma parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available actions given a state\n",
    "def available_actions(state):\n",
    "    curr_state_row = R[state, ]\n",
    "    av_act = np.where(curr_state_row >= 0)[1]\n",
    "    return av_act\n",
    "\n",
    "available_act = available_actions(initial_state)\n",
    "\n",
    "\n",
    "#randomly choose action among available\n",
    "def sample_next_action(avl_acts):\n",
    "    next_action = int(np.random.choice(avl_acts, 1))\n",
    "    return next_action\n",
    "\n",
    "next_act = sample_next_action(available_act)\n",
    "\n",
    "#This method updates Q matrix based on path choosen and Q Learning algo\n",
    "def update(current_state, action, gamma):\n",
    "    #print(action)\n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[0]\n",
    "    \n",
    "    if max_index.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(max_index, 1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "        \n",
    "    max_value = Q[action, max_index]\n",
    "    \n",
    "    #Q Learning Formula\n",
    "    Q[current_state, action] = R[current_state, action] + gamma * max_value\n",
    "\n",
    "#Update Q matrix\n",
    "update(initial_state, next_act, gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined the utility methods, it is time to play this game a lot of times from random initial states. The Q Matrix is updated with all these iterations of game play and would represent the incremental rewards of taking any particular action at a given state. This Q matrix represents the model which shall be used when the\n",
    "agent plays the game during testing phase. \n",
    "\n",
    "But before training , let us play the game with default (all Zero) Q Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "#%% Testing Phase with Default Q Matrix\n",
    "test_state = 2\n",
    "steps = [test_state]\n",
    "while(test_state != 5):\n",
    "    next_step_index = np.where(Q[test_state, ] == np.max(Q[test_state,]))[0]\n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, 1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "    \n",
    "    steps.append(next_step_index)\n",
    "    test_state = next_step_index\n",
    "\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the steps shown above might not be even valid, as once trained we will only be referring to Q Matrix to make our decisions. Since Q matrix is all zero as of now, all state transitions are equally likely. Simply speaking the model does not know the rules of the game as of now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Training\n",
    "for i in range(10000):\n",
    "    initial_state = int(np.random.randint(0, Q.shape[0]))\n",
    "    available_act = available_actions(initial_state)\n",
    "    action = sample_next_action(available_act)\n",
    "    update(initial_state, action, gamma)\n",
    "\n",
    "\n",
    "#Normalize Q Matrix\n",
    "Q = Q / np.max(Q) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us show the model in action now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "#%% Testing Phase\n",
    "test_state = 2\n",
    "steps = [test_state]\n",
    "while(test_state != 5):\n",
    "    next_step_index = np.where(Q[test_state, ] == np.max(Q[test_state,]))[0]\n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, 1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "    \n",
    "    steps.append(next_step_index)\n",
    "    test_state = next_step_index\n",
    "\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path suggested by the model, after training is the best possible path. This completes the introduction of the Q Learn algorithm. \n",
    "\n",
    "After Thoughts:\n",
    "This was a simple toy example, where only a small number of states were possible. Therefore we were able to describe the complete list of prescribed actions fairly easily. In most scenarios the number of states are huge and creating fully descriptive Q matrix is mostly impracticle for them. For example a tic tac toe game, which is again a very simple example can have 19683(3^9) possible states. As Per [2], Chess has 10^123 possible games, while Go has 10^360 possible games. In these scenerios, some plausible workarounds are:\n",
    "1. we create Q matrix for a small number of steps ahead instead of end of game. \n",
    "2. Simulating large number of games using Monte Carlo Simulations and check what percetanges of games were won (or lost) from a given board position (State). \n",
    "3. Discounted Rewards may be applied backwards to a action that resulted in Reward. \n",
    "4. Deep Neural Networks are very common in predicting in the next move, if lot of game playing data is available. \n",
    "Some interesting resources to learn about advanced RL techniques are [7] & [8]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "1. Edureka Video on Reinforcement Learning: https://www.youtube.com/watch?v=LzaWrmKL1Z4\n",
    "2. Tic Tac Toe Project using Reinforcement Learning: https://danielslater.net/2016/10/01/alphatoe/\n",
    "3. Deepmind's Alpha Go: https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "4. Chess Project using reinforcement leanring: https://github.com/Zeta36/chess-alpha-zero\n",
    "5. Mario Game Reinforcement Learning: https://www.cse.iitb.ac.in/~nilesh/media/Mario_Neat.pdf\n",
    "6. Superman Fantasy AI: https://superman.fandom.com/wiki/Jor-El_A.I\n",
    "7. Spinning Up: https://spinningup.openai.com/en/latest/index.html\n",
    "8. Blog: https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
